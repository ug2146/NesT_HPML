name: nest-tiny-analog-inferencerpu
training_type: analog   # default -> baseline training / analog -> analog training / quant -> quantization aware training
model: nest-tiny-analog # nest-tiny -> default model / nest-msa-t -> MSA module / nest-tiny-analog -> analog model
dataset: cifar-10

ngpus: 1
train_batch_size: 192
test_batch_size: 100
num_workers: 8
nepochs: 100
optimizer: AdamW    # AdamW -> default models / AnalogAdam -> analog models
warmup_scheduler: LinearLR
train_scheduler: CosineAnnealingLR
warmup_epochs: 5
warmup_lr: 2.5e-3
lr: 1.0e-2
lr_min: 5.0e-3  # for cosine annealing scheduler
lr_restart: 100 # for cosine annealing scheduler
weight_decay: 0.05
loss: ce-loss

save_every: 50  # save model in these intervals

resume: False
verbose: False

run_type: 'individual' # 'individual/sweep'
sweep_parameters:
  # preset:
  # - 'ReRamESPreset'
  # - 'PCMPreset' 
  # - 'EcRamMO2Preset'
  # - 'ReRamSB4Preset'
  # - 'TikiTakaEcRamPreset'
  # weight_scaling_omega:
  #   min: 0.0
  #   max: 10.0
  # learn_out_scaling:
  #   - True
  #   - False
